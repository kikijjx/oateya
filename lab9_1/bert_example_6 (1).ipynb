{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zTjybYzLxPvv"
   },
   "source": [
    "**PART 1: Получение устройства для расчётов**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oM17eWNSxPvw",
    "outputId": "c04253f1-4f20-48f9-f067-a76d949d5eb4"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_device\u001b[39m():\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m# Если в системе есть GPU ...\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m      7\u001b[0m         \u001b[38;5;66;03m# Тогда говорим PyTorch использовать GPU.\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    # Если в системе есть GPU ...\n",
    "    if torch.cuda.is_available():\n",
    "        # Тогда говорим PyTorch использовать GPU.\n",
    "        device = torch.device(\"cuda\")\n",
    "        print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "        print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "    # Если нет GPU, то считаем на обычном процессоре ...\n",
    "    else:\n",
    "        print('No GPU available, using the CPU instead.')\n",
    "        device = torch.device(\"cpu\")\n",
    "    return device\n",
    "\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b7alEhi3xPvx"
   },
   "source": [
    "**PART 2: Загрузка датасета из интернета**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "KSLJlltqxPvx",
    "outputId": "9d4223f0-c2f5-4ae8-ba16-63aae62871c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting wget\n",
      "  Using cached wget-3.2-py3-none-any.whl\n",
      "Installing collected packages: wget\n",
      "Successfully installed wget-3.2\n",
      "Downloading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Complete\n"
     ]
    }
   ],
   "source": [
    "!pip install wget\n",
    "\n",
    "def download_dataset():\n",
    "    import wget\n",
    "    import os\n",
    "    import zipfile\n",
    "\n",
    "    print('Downloading dataset...')\n",
    "    # URL до zip-файла который содержит датасет.\n",
    "    url = 'https://nyu-mll.github.io/CoLA/cola_public_1.1.zip'\n",
    "    out_file = './cola_public_1.1.zip'\n",
    "\n",
    "    # Скачиваем файл (только в случае если не скачали раньше)\n",
    "    if not os.path.exists(out_file):\n",
    "        wget.download(url, out_file)\n",
    "    # Unzip\n",
    "    if not os.path.exists('./cola_public/'):\n",
    "        with zipfile.ZipFile(out_file, 'r') as zip_ref:\n",
    "            zip_ref.extractall(os.path.dirname(out_file))\n",
    "    print('Complete')\n",
    "\n",
    "\n",
    "download_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "60xjCMb6xPvy"
   },
   "source": [
    "**PART 3: Получение предложений и разметки к ним**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uf8EbHiXxPvy",
    "outputId": "12bcf24c-788e-429f-eaf4-073e31abee4d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of reviews: 42,656\n",
      "\n",
      "       Review_ID  Rating Year_Month Reviewer_Location  \\\n",
      "22534  195628345       3     2014-2            Canada   \n",
      "35024  323698304       3    2015-10    United Kingdom   \n",
      "42558    5951471       5    missing    United Kingdom   \n",
      "8348   149419922       4     2013-1       Philippines   \n",
      "41904  116839317       1     2011-8            Cyprus   \n",
      "11496  555578560       5     2018-1     United States   \n",
      "34151  366035133       5    2015-12    United Kingdom   \n",
      "8275   152276956       5    2012-11             India   \n",
      "4716   343749039       4    2015-11          Malaysia   \n",
      "33433  404793768       4     2016-8    United Kingdom   \n",
      "\n",
      "                                             Review_Text  \\\n",
      "22534  This is my second visit. In the off season a l...   \n",
      "35024  Let   s just imagine for a minute the meeting ...   \n",
      "42558  My boyfriend and I went on the 24th 28th Septe...   \n",
      "8348   Sure this Disneyland isn't the biggest, but if...   \n",
      "41904  The rating is for this ride specifically! Our ...   \n",
      "11496  Be prepared to walk a lot. I suggest to go ear...   \n",
      "34151  We have been to 4 of the worlds 5 Disney lands...   \n",
      "8275   Decided to take my son who's turned 7 to Hong ...   \n",
      "4716   I'm a big fan of Disney and the visit to Disne...   \n",
      "33433  Visited as a family of 4 with 13 and 9.year ol...   \n",
      "\n",
      "                      Branch  \n",
      "22534  Disneyland_California  \n",
      "35024       Disneyland_Paris  \n",
      "42558       Disneyland_Paris  \n",
      "8348     Disneyland_HongKong  \n",
      "41904       Disneyland_Paris  \n",
      "11496  Disneyland_California  \n",
      "34151       Disneyland_Paris  \n",
      "8275     Disneyland_HongKong  \n",
      "4716     Disneyland_HongKong  \n",
      "33433       Disneyland_Paris  \n",
      "High ratings:\n",
      "                                             Review_Text  Rating\n",
      "42611  We got back from Disneyland Paris yesterday an...       5\n",
      "18390  Family trip from Texas. We enjoyed our day at ...       4\n",
      "13579  Amazing place, even in the rain! Unfortunately...       4\n",
      "20998  This was the first time we have visited Disney...       5\n",
      "16374  FirstlyI love Disney, which had all the usual ...       4\n",
      "\n",
      "Low ratings:\n",
      "                                             Review_Text  Rating\n",
      "29023  never again...what a horrible experience, the ...       1\n",
      "27917  My family and I visited Disneyland on November...       2\n",
      "9194   If disney was going to do this then they shoul...       2\n",
      "35788  Being from Canada I'm not use to so many diffe...       1\n",
      "36333  The only one positive thing   availability of ...       2\n",
      "[2 2 2 ... 3 2 2]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "def get_sentences_and_labels():\n",
    "    dataset_path = \"./DisneylandReviews.csv\"\n",
    "    # Загружаем датасет\n",
    "    df = pd.read_csv(dataset_path, encoding='cp1251')\n",
    "    print('Number of reviews: {:,}\\n'.format(df.shape[0]))\n",
    "    print(df.sample(10))\n",
    "\n",
    "    print(\"High ratings:\")\n",
    "    print(df.loc[df.Rating >= 4].sample(5)[['Review_Text', 'Rating']])\n",
    "    print(\"\\nLow ratings:\")\n",
    "    print(df.loc[df.Rating <= 2].sample(5)[['Review_Text', 'Rating']])\n",
    "\n",
    "    sentences = df['Review_Text'].values\n",
    "    labels = df['Rating'].values\n",
    "    labels = labels - 2\n",
    "    print(labels)\n",
    "\n",
    "    return sentences, labels\n",
    "\n",
    "\n",
    "sentences, labels = get_sentences_and_labels()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sWF-cI9jxPvy"
   },
   "source": [
    "**PART 4: Получение токенайзера и тестирование его работы**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Ied6tGSxPvy",
    "outputId": "c1aa124a-2bda-4e07-eacf-01ee5c5cd8c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading BERT tokenizer...\n",
      "Original: If you've ever been to Disneyland anywhere you'll find Disneyland Hong Kong very similar in the layout when you walk into main street! It has a very familiar feel. One of the rides  its a Small World  is absolutely fabulous and worth doing. The day we visited was fairly hot and relatively busy but the queues moved fairly well. \n",
      "Tokenized:  ['if', 'you', \"'\", 've', 'ever', 'been', 'to', 'disneyland', 'anywhere', 'you', \"'\", 'll', 'find', 'disneyland', 'hong', 'kong', 'very', 'similar', 'in', 'the', 'layout', 'when', 'you', 'walk', 'into', 'main', 'street', '!', 'it', 'has', 'a', 'very', 'familiar', 'feel', '.', 'one', 'of', 'the', 'rides', 'its', 'a', 'small', 'world', 'is', 'absolutely', 'fabulous', 'and', 'worth', 'doing', '.', 'the', 'day', 'we', 'visited', 'was', 'fairly', 'hot', 'and', 'relatively', 'busy', 'but', 'the', 'queue', '##s', 'moved', 'fairly', 'well', '.']\n",
      "Token IDs:  [2065, 2017, 1005, 2310, 2412, 2042, 2000, 25104, 5973, 2017, 1005, 2222, 2424, 25104, 4291, 4290, 2200, 2714, 1999, 1996, 9621, 2043, 2017, 3328, 2046, 2364, 2395, 999, 2009, 2038, 1037, 2200, 5220, 2514, 1012, 2028, 1997, 1996, 12271, 2049, 1037, 2235, 2088, 2003, 7078, 18783, 1998, 4276, 2725, 1012, 1996, 2154, 2057, 4716, 2001, 7199, 2980, 1998, 4659, 5697, 2021, 1996, 24240, 2015, 2333, 7199, 2092, 1012]\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer\n",
    "print('Loading BERT tokenizer...')\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "sentence_number = 0\n",
    "# Напечатать оригинальное предложение.\n",
    "print('Original:', sentences[sentence_number])\n",
    "# Напечатать предложение разбитое на отдельные токены из словаря.\n",
    "print('Tokenized: ', tokenizer.tokenize(sentences[sentence_number]))\n",
    "# Напечатать предложение разбитое на номера токенов в словаре.\n",
    "print('Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(sentences[sentence_number])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RP2n4A7NxPvy"
   },
   "source": [
    "**PART 5: Подсчет максимальной длины текста в датасете (с учетом токенизации и специальных токенов)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xnHl5ooRxPvz",
    "outputId": "2c003531-89b3-4376-ec03-17f14d418ca6"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (651 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max sentence length:  4768\n"
     ]
    }
   ],
   "source": [
    "max_len = 0\n",
    "# Считаем какой максимальный размер имеет предложение разбитое на токены и разбавленное спец. токенами.\n",
    "for sent in sentences:\n",
    "    # Токенизируем текст и добавляем `[CLS]` и `[SEP]` токены.\n",
    "    input_ids = tokenizer.encode(sent, add_special_tokens=True)\n",
    "    # Обновляем максимум.\n",
    "    max_len = max(max_len, len(input_ids))\n",
    "print('Max sentence length: ', max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vWSgatK9xPvz"
   },
   "source": [
    "**PART 6: Токенизация всех предложений в датасете (полноценно)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_9WIYFkJxPvz",
    "outputId": "ad37a8db-e055-4306-b420-0a56ebabb886"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original:  If you've ever been to Disneyland anywhere you'll find Disneyland Hong Kong very similar in the layout when you walk into main street! It has a very familiar feel. One of the rides  its a Small World  is absolutely fabulous and worth doing. The day we visited was fairly hot and relatively busy but the queues moved fairly well. \n",
      "Token IDs: tensor([  101,  2065,  2017,  1005,  2310,  2412,  2042,  2000, 25104,  5973,\n",
      "         2017,  1005,  2222,  2424, 25104,  4291,  4290,  2200,  2714,  1999,\n",
      "         1996,  9621,  2043,  2017,  3328,  2046,  2364,  2395,   999,  2009,\n",
      "         2038,  1037,  2200,  5220,  2514,  1012,  2028,  1997,  1996, 12271,\n",
      "         2049,  1037,  2235,  2088,  2003,  7078, 18783,  1998,  4276,  2725,\n",
      "         1012,  1996,  2154,  2057,  4716,  2001,  7199,  2980,  1998,  4659,\n",
      "         5697,  2021,  1996, 24240,  2015,  2333,  7199,  2092,  1012,   102,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
      "            0,     0,     0,     0,     0,     0,     0,     0])\n",
      "Attention masks: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Labels: tensor(2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nikita\\AppData\\Local\\Temp\\ipykernel_2848\\633118731.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  labels = torch.tensor(labels)\n"
     ]
    }
   ],
   "source": [
    "input_ids, attention_masks = [], []\n",
    "\n",
    "# Для всех предложений...\n",
    "for sent in sentences:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        sent,  # Текст для токенизации.\n",
    "        add_special_tokens=True,  # Добавляем '[CLS]' и '[SEP]'\n",
    "        max_length=128,  # Дополняем [PAD] или обрезаем текст до 64 токенов.\n",
    "        pad_to_max_length=True,\n",
    "        return_attention_mask=True,  # Возвращаем также attn. masks.\n",
    "        return_tensors='pt',  # Возвращаем в виде тензоров pytorch.\n",
    "    )\n",
    "\n",
    "    # Добавляем токенизированное предложение в список\n",
    "    input_ids.append(encoded_dict['input_ids'])\n",
    "    # И добавляем attention mask в список\n",
    "    attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "# Конвертируем списки в полноценные тензоры Pytorch.\n",
    "input_ids = torch.cat(input_ids, dim=0)\n",
    "attention_masks = torch.cat(attention_masks, dim=0)\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# Печатаем предложение с номером 0, его токены (теперь в виде номеров в словаре) и.т.д.\n",
    "print('Original: ', sentences[0])\n",
    "print('Token IDs:', input_ids[0])\n",
    "print('Attention masks:', attention_masks[0])\n",
    "print('Labels:', labels[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lCrPWrV3xPvz"
   },
   "source": [
    "**PART 7: Разделение данных на тренировочные и валидационные**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SRbcN6mMxPvz",
    "outputId": "177ac3dd-415d-45d7-e23e-3a4f98006a60"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38,390 training samples\n",
      "4,266 validation samples\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import TensorDataset, random_split\n",
    "\n",
    "# Объединяем все тренировочные данные в один TensorDataset.\n",
    "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
    "\n",
    "# Делаем разделение случайное разбиение 90% - тренировка 10% - валидация.\n",
    "\n",
    "# Считаем число данных для тренировки и для валидации.\n",
    "train_size = int(0.9 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "\n",
    "# Разбиваем датасет с учетом посчитанного количества.\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "print('{:>5,} training samples'.format(train_size))\n",
    "print('{:>5,} validation samples'.format(val_size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OlOmrj7RxPv0"
   },
   "source": [
    "**PART 8: Создание загрузчиков данных (Data Loaders)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bj4kdQDqxPv0"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "\n",
    "# DataLoader должен знать размер батча для тренировки мы задаем его здесь.\n",
    "# Размер батча – это сколько текстов будет подаваться на сеть для вычисления градиентов\n",
    "# Авторы BERT предлагают ставить его 16 или 32.\n",
    "batch_size = 32\n",
    "\n",
    "# Создаем отдельные DataLoaders для наших тренировочного и валидационного наборов\n",
    "\n",
    "# Для тренировки мы берем тексты в случайном порядке.\n",
    "train_dataloader = DataLoader(\n",
    "        train_dataset,  # Тренировочный набор данных.\n",
    "        sampler = RandomSampler(train_dataset), # Выбираем батчи случайно\n",
    "        batch_size = batch_size # Тренируем с таким размером батча.\n",
    ")\n",
    "\n",
    "# Для валидации порядок не важен, поэтому зачитываем их последовательно.\n",
    "validation_dataloader = DataLoader(\n",
    "        val_dataset, # Валидационный набор данных.\n",
    "        sampler = SequentialSampler(val_dataset), # Выбираем батчи последовательно.\n",
    "        batch_size = batch_size # Считаем качество модели с таким размером батча.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h8J4VjVNxPv0"
   },
   "source": [
    "**PART 9: Создаем модель BERT и выводим структуру её слоёв для примера**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 498
    },
    "id": "f3uOxLTzxPv0",
    "outputId": "724da8cf-172c-4369-82c9-43f1db55dfbd"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The BERT model has 201 different named parameters.\n",
      "\n",
      "==== Embedding Layer ====\n",
      "\n",
      "bert.embeddings.word_embeddings.weight                  (30522, 768)\n",
      "bert.embeddings.position_embeddings.weight                (512, 768)\n",
      "bert.embeddings.token_type_embeddings.weight                (2, 768)\n",
      "bert.embeddings.LayerNorm.weight                              (768,)\n",
      "bert.embeddings.LayerNorm.bias                                (768,)\n",
      "\n",
      "==== First Transformer ====\n",
      "\n",
      "bert.encoder.layer.0.attention.self.query.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.query.bias                (768,)\n",
      "bert.encoder.layer.0.attention.self.key.weight            (768, 768)\n",
      "bert.encoder.layer.0.attention.self.key.bias                  (768,)\n",
      "bert.encoder.layer.0.attention.self.value.weight          (768, 768)\n",
      "bert.encoder.layer.0.attention.self.value.bias                (768,)\n",
      "bert.encoder.layer.0.attention.output.dense.weight        (768, 768)\n",
      "bert.encoder.layer.0.attention.output.dense.bias              (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.weight        (768,)\n",
      "bert.encoder.layer.0.attention.output.LayerNorm.bias          (768,)\n",
      "bert.encoder.layer.0.intermediate.dense.weight           (3072, 768)\n",
      "bert.encoder.layer.0.intermediate.dense.bias                 (3072,)\n",
      "bert.encoder.layer.0.output.dense.weight                 (768, 3072)\n",
      "bert.encoder.layer.0.output.dense.bias                        (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.weight                  (768,)\n",
      "bert.encoder.layer.0.output.LayerNorm.bias                    (768,)\n",
      "\n",
      "==== Output Layer ====\n",
      "\n",
      "bert.pooler.dense.weight                                  (768, 768)\n",
      "bert.pooler.dense.bias                                        (768,)\n",
      "classifier.weight                                           (5, 768)\n",
      "classifier.bias                                                 (5,)\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
    "\n",
    "# Загружаем BertForSequenceClassification. Это предобученная модель BERT с одиночным полносвязным слоем для классификации\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\", # Используем 12-слойную модель BERT, со словарем без регистра.\n",
    "    num_labels = 5, # Количество выходных слоёв – 2 для бинарной классификации. Можно увеличить для мультиклассовой классификации.\n",
    "    output_attentions = False, # Будет ли модель возвращать веса для attention-слоёв. В нашем случае нет.\n",
    "    output_hidden_states = False, # Будет ли модель возвращать состояние всех скрытых слоёв. В нашем случае нет.\n",
    ")\n",
    "\n",
    "# Здесь мы говорим PyTorch что хотим тренировать модель на GPU.\n",
    "if torch.cuda.is_available():\n",
    "    model.cuda()\n",
    "\n",
    "# Получаем все параметры модели как список кортежей и выводим сводную информацию по модели.\n",
    "params = list(model.named_parameters())\n",
    "print('The BERT model has {:} different named parameters.\\n'.format(len(params)))\n",
    "print('==== Embedding Layer ====\\n')\n",
    "for p in params[0:5]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== First Transformer ====\\n')\n",
    "for p in params[5:21]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n",
    "\n",
    "print('\\n==== Output Layer ====\\n')\n",
    "for p in params[-4:]:\n",
    "    print(\"{:<55} {:>12}\".format(p[0], str(tuple(p[1].size()))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R9YedwvcxPv0"
   },
   "source": [
    "**PART 10: Создаем оптимизатор Adam, задаем количество эпох для тренировки и создаем планировщик learning rate**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0v0_pLMCxPv0",
    "outputId": "3d2c2909-58e6-4596-a0e6-8344c1685dcc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\nikita\\Desktop\\vscode\\oateya\\lab9_1\\venv\\Lib\\site-packages\\transformers\\optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "    lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
    "    eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
    ")\n",
    "\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# Количество эпох для тренировки. Авторы BERT рекомендуют от 2 до 4.\n",
    "# Мы выбираем 4, но увидим позже, что это приводит к оверфиту на тренировочные данные.\n",
    "epochs = 2\n",
    "\n",
    "# Общее число шагов тренировки равно [количество батчей] x [число эпох].\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "\n",
    "# Создаем планировщик learning rate (LR). LR будет плавно уменьшаться в процессе тренировки\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
    "                                            num_training_steps = total_steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j_S3jF_ExPv0"
   },
   "source": [
    "**PART 11: Две полезные функции: расчёт точности и вывод затраченного времени**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P6d_UYivxPv0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Функция для расчёта точности. Сравниваются предсказания и реальная разметка к данным\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1)\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "# На вход время в секундах и возвращается строка в формате hh:mm:ss\n",
    "def format_time(elapsed):\n",
    "    # Округляем до ближайшей секунды.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "\n",
    "    # Форматируем как hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJYH7kdMxPv1"
   },
   "source": [
    "**PART 12: Выполняем один проход обучения по всем тренировочным данным.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pazmtfR4xPv1"
   },
   "outputs": [],
   "source": [
    "def train_step(device, model, train_dataloader, optimizer, scheduler):\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    t0 = time.time()\n",
    "    total_train_loss = 0\n",
    "    # Переводим модель в режим тренировки.\n",
    "    model.train()\n",
    "\n",
    "    # Для каждого батча из тренировочных данных...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "        # Извлекаем все компоненты из полученного батча\n",
    "        b_input_ids, b_input_mask, b_labels = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
    "        # Очищаем все ранее посчитанные градиенты (это важно)\n",
    "        model.zero_grad()\n",
    "        # Выполняем прямой проход по данным\n",
    "        #loss, logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss = outputs.loss\n",
    "        logits = outputs.logits\n",
    "        # Накапливаем тренировочную функцию потерь по всем батчам\n",
    "        total_train_loss += loss.item()\n",
    "        # Выполняем обратное распространение ошибки что бы посчитать градиенты.\n",
    "        loss.backward()\n",
    "        # Ограничиваем максимальный размер градиента до 1.0. Это позволяет избежать проблемы \"exploding gradients\".\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "        # Обновляем параметры модели используя рассчитанные градиенты с помощью выбранного оптимизатора и текущего learning rate.\n",
    "        optimizer.step()\n",
    "        # Обновляем learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Считаем среднее значение функции потерь по всем батчам.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "    # Сохраняем время тренировки одной эпохи.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "    return avg_train_loss, training_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zk_l-zTQxPv1"
   },
   "source": [
    "**PART 14: Выполняем один проход подсчёта метрик на валидации**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Iqw0ixsExPv1"
   },
   "outputs": [],
   "source": [
    "def validation_step(device, model, validation_dataloader):\n",
    "    print(\"Running Validation...\")\n",
    "    t0 = time.time()\n",
    "    # Переводим модель в режим evaluation – некоторые слои, например dropout ведут себя по другому.\n",
    "    model.eval()\n",
    "\n",
    "    # Переменные для подсчёта функции потерь и точности\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    # Прогоняем все данные из валидации\n",
    "    for batch in validation_dataloader:\n",
    "        # Извлекаем все компоненты из полученного батча.\n",
    "        b_input_ids, b_input_mask, b_labels = batch[0].to(device), batch[1].to(device), batch[2].to(device)\n",
    "\n",
    "        # Говорим pytorch что нам не нужен вычислительный граф для подсчёта градиентов (всё будет работать намного быстрее)\n",
    "        with torch.no_grad():\n",
    "            # Прямой проход по нейронной сети и получение выходных значений.\n",
    "            outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "            loss = outputs.loss\n",
    "            logits = outputs.logits\n",
    "            #(loss, logits) = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "\n",
    "        # Накапливаем значение функции потерь для валидации.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Переносим значения с GPU на CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        # Считаем точность для отдельного батча с текстами и накапливаем значения.\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "\n",
    "    # Выводим точность для всех валидационных данных.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Считаем среднюю функцию потерь для всех батчей.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    # Измеряем как долго считалась валидация.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "    return avg_val_loss, avg_val_accuracy, validation_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WhC79bslxPv1"
   },
   "source": [
    "**PART 15: Основной цикл тренировки**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 506
    },
    "id": "yfJmak_9xPv1",
    "outputId": "3a789c7b-c311-41ce-cd87-80e5324cd427"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======== Epoch 1 / 2 ========\n"
     ]
    }
   ],
   "source": [
    "# В этой переменной сохраним всякую статистику по тренировке: точность, функцию цены (потерь) и время выполнения.\n",
    "training_stats = []\n",
    "# Переменная что бы измерить время всей тренировки.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# Для каждой эпохи...\n",
    "for epoch_i in range(0, epochs):\n",
    "    # Запустить одну эпоху тренировки (следующий слайд)\n",
    "    avg_train_loss, training_time = train_step(device, model, train_dataloader, optimizer, scheduler)\n",
    "    # Запустить валидацию что бы проверить качество модели на данном этапе (следующий слайд)\n",
    "    avg_val_loss, avg_val_accuracy, validation_time = validation_step(device, model, validation_dataloader)\n",
    "\n",
    "    # Сохраняем статистику тренировки на данной эпохе.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'Epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Validation Loss': avg_val_loss,\n",
    "            'Validation Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"Training complete! Total training took {:} (hh:mm:ss)\".format(format_time(time.time() - total_t0)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mJGrqapMxPv1"
   },
   "source": [
    "**PART 15: Сохранение модели**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j7NE_7j1xPv1",
    "outputId": "0a881f4b-444c-44cb-ffe7-520f3c06b8e0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model to ./model_save/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('./model_save/tokenizer_config.json',\n",
       " './model_save/special_tokens_map.json',\n",
       " './model_save/vocab.txt',\n",
       " './model_save/added_tokens.json')"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Задаем выходную директорию\n",
    "output_dir = './model_save/'\n",
    "# Если она не существует создаем её\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "print(\"Saving model to %s\" % output_dir)\n",
    "\n",
    "# Сохраняем натренированную модель и её токенайзер используя `save_pretrained()`.\n",
    "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
    "model_to_save.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RJNaFxtXxPv2"
   },
   "source": [
    "**PART 16: Восстановление модели из сохраненной копии**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kTSRheR1xPv2"
   },
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "# Загружаем натренированную модель и её словарь\n",
    "model = BertForSequenceClassification.from_pretrained(output_dir)\n",
    "tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "# Отправляем модель на GPU.\n",
    "if torch.cuda.is_available():\n",
    "    model.to(device)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "papermill": {
   "duration": 2514.197238,
   "end_time": "2020-09-24T08:34:15.204249",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-09-24T07:52:21.007011",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
